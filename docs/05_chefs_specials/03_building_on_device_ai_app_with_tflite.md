# ケーススタディ#5-3: オンデバイスAIで画像認識アプリを作る (TFLite)

これまでのケーススタディでは、クラウド上のAIモデル（GeminiやAPI）と通信するアプリを作りました。この最終章では、AIモデルそのものをアプリに直接組み込み、**インターネット接続なしで（オフラインで）動作する**、高度な画像認識アプリを構築します。

この技術を**オンデバイスAI**と呼び、Flutterでは**TensorFlow Lite (TFLite)** というフレームワークを使って実現します。

## このレシピで作るもの

*   カメラで撮影した写真や、ギャラリーから選んだ画像をアプリに表示する。
*   ボタンをタップすると、アプリに組み込まれたAIモデルが画像を分析し、「これは猫です（95%の確率）」のように、画像に写っている物体が何かを判定して表示する。

![画像認識アプリのイメージ](https://miro.medium.com/v2/resize:fit:1400/1*y3z2k_5c_m55_V3okn9x4w.png)
*(Image credit: medium.com)*

## Step 1: AIモデルとラベルファイルの準備

オンデバイスAIでは、まず「学習済みのAIモデル」と、モデルが何を認識できるかを示した「ラベル」のリストを用意する必要があります。

1.  **モデルのダウンロード:**
    *   [TensorFlow Hub](https://tfhub.dev/)のようなサイトには、画像分類、物体検出など、様々な目的のために事前学習されたTFLiteモデルが無料で公開されています。
    *   今回は、一般的な物体を1000種類以上認識できる[**`MobileNetV2`**](https://tfhub.dev/tensorflow/lite-model/mobilenet_v2_1.0_224/1/metadata/1)のような、軽量でモバイルに適したモデルをダウンロードします。
    *   ダウンロードしたファイルは、通常`.tflite`という拡張子です。

2.  **ラベルファイルの準備:**
    *   モデルが認識できる物体の名前が、一行ずつ書かれたテキストファイルです（例: `labels.txt`）。
    *   モデルのダウンロードページに、対応するラベルファイルも提供されていることがほとんどです。

3.  **プロジェクトへの配置:**
    *   Flutterプロジェクトのルートに`assets`フォルダを作成します。
    *   ダウンロードした`.tflite`モデルファイルと`labels.txt`ファイルを、この`assets`フォルダに配置します。
    *   `pubspec.yaml`ファイルに、これらのアセットファイルを登録するのを忘れないでください。
        ```yaml
        flutter:
          assets:
            - assets/model.tflite
            - assets/labels.txt
        ```

## Step 2: AIにTFLiteの連携コードを生成させる

Flutterアプリから`.tflite`モデルを操作するには、専用のパッケージが必要です。ここでは、コミュニティで広く使われている`tflite_flutter`（または、より高レベルな`flutter_tflite`）を使います。

> **🤖 AI活用プロンプト (TFLite連携コード)**
>
> あなたは、FlutterとTensorFlow Liteを連携させるオンデバイスAIアプリの開発に非常に詳しいエキスパートです。
>
> `flutter_tflite`パッケージを使って、画像分類を行うための基本的なサービスクラスを作成してください。
>
> **要件:**
> 1.  `loadModel()`メソッド: `assets`から`model.tflite`と`labels.txt`を読み込み、モデルを初期化する。
> 2.  `runModelOnImage(String imagePath)`メソッド:
>     - 引数として画像のパスを受け取る。
>     - 画像をモデルが要求する形式（例: 224x224ピクセル）にリサイズする。
>     - 画像をモデルに入力し、推論（`runModelOnImage`）を実行する。
>     - 推論結果（認識されたオブジェクトのリストと、それぞれの信頼度スコア）を返す。
> 3.  `closeModel()`メソッド: アプリ終了時にモデルを解放する。
>
> このクラス（例: `TfliteService`）の完全なコードを提示してください。

**なぜこれが良いのか？**
*   **専門知識の塊:** TFLiteの連携には、画像のバイトデータへの変換、テンソル（多次元配列）の操作など、専門的な知識が必要です。AIは、これらの複雑で定型的な処理をすべて肩代わりしてくれます。
*   **非同期処理の管理:** モデルの読み込みや推論は、UIをブロックしないように非同期で行う必要があります。AIは、`async/await`を適切に使ったコードを生成します。

## Step 3: UIと推論ロジックの結合

次に、ユーザーが画像を選択し、推論を実行するためのUIを作成し、先ほど作成した`TfliteService`と連携させます。

> **🤖 AI活用プロンプト (UIとロジックの結合)**
>
> `image_picker`パッケージで画像を選択し、`TfliteService`で画像認識を行う画面を作成してください。
>
> **要件:**
> 1.  画面には、選択された画像を表示する領域と、「画像を選択」ボタンを配置する。
> 2.  画像が表示されている状態で「この画像を認識」ボタンを配置する。
> 3.  「この画像を認識」ボタンが押されたら、`TfliteService.runModelOnImage()`を呼び出す。
> 4.  推論中は、ローディングインジケーターを表示する。
> 5.  推論が完了したら、結果（例: `Cat: 95.8%`）を画面下部にリスト表示する。
> 6.  この画面の状態管理は、Riverpodを使って実装してください。

このプロンプトにより、画像選択、非同期でのAI推論実行、そして結果の表示という、一連の流れを管理するUIとロジックのコードが生成されます。

## Step 4: パフォーマンスと精度の調整

オンデバイスAIは、クラウドAIと比べてリソースの制約が大きいため、最後の調整が重要になります。

*   **推論速度:**
    *   もし推論が遅い場合、AIに「もっと軽量なモデルはないか？」「画像の解像度を下げて入力するとどうなるか？」といった相談ができます。
*   **認識精度:**
    *   AIに「推論結果の信頼度スコアが低いもの（例: 50%未満）は、結果として表示しないようにロジックを修正して」と依頼することで、ユーザーに不確かな情報を見せないようにできます。
*   **モデルの更新:**
    *   より新しい、または特定のタスクに特化したモデルが見つかった場合、`assets`内のファイルを差し替え、AIに「新しいモデルの入力/出力形式に合わせて、コードを修正して」と依頼するだけで、簡単にAIの頭脳をアップグレードできます。

このケーススタディを通じて、あなたはFlutterアプリに「知能」そのものを埋め込むという、最先端の技術を体験しました。オフラインでも動作する高速なAI機能は、あなたのアプリにユニークで強力な価値をもたらすでしょう。